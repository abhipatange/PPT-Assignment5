{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c49c2d-de0c-4bb7-bde3-b1c427155b50",
   "metadata": {},
   "source": [
    "Naive Approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a605fa-9b18-4f24-977e-856686ce2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the Naive Approach in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8e1e4-c017-4c66-b341-c22640bbdc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = A naive classifier model is one that does not use any sophistication in order to make a prediction,\n",
    "typically making a random or constant prediction.\n",
    "Such models are naive because they dont use any \n",
    "knowledge about the domain or any learning in order to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039978a-b298-4103-a1ea-1e949f291399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b35bc4-38f6-4a39-893f-b47058ce07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain the assumptions of feature independence in the Naive Approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef3d318-84a4-4ccb-8534-590e83ecb815",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = The Naive Approach, also known as the Naive Bayes classifier,\n",
    "makes the assumption of feature independence.\n",
    "This assumption states that the features used in the classification are conditionally\n",
    "independent of each other given the class label.\n",
    "In other words, it assumes that the presence or absence of a particular feature does not \n",
    "affect the presence or absence of any other feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0799d36-7f5d-4a16-ade5-e36c50f6581c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a9766-f111-41de-acbb-6b3b8b0ddea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How does the Naive Approach handle missing values in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba05bd-1a13-494c-be62-7ccf3488c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Naïve Bayes Imputation  is used to fill in missing values by \n",
    "replacing the attribute information according to the probability estimate.\n",
    "The NBI process divides the whole data into two sub-sets is the complete data \n",
    "and data containing missing data.\n",
    "Complete data is used for the imputation process at the lost value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3cbd66-b4fd-46f7-9fe5-b249a8c6c0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4e6b4-fa19-441e-8bc9-ae23b15c09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What are the advantages and disadvantages of the Naive Approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a2b4f-823c-4321-86e0-45bbb41cb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = The advantage is that it is inexpensive to develop, store data, \n",
    "and operate. The disadvantage is that it does not consider any possible\n",
    "causal relationships that underly the forecasted variable.\n",
    "This model adds the latest observed absolute period -to-period\n",
    "change to the most recent observed level of the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71566a-a2d7-4931-b72f-4162881b1f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617372a3-dff2-457c-ba87-ae20c1df4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Can the Naive Approach be used for regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce5963-ee5d-47f2-ac83-517b6abcad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Yes.\n",
    "Naive Bayes is a supervised classification algorithm that is used primarily for\n",
    "dealing with binary and multi-class classification problems, \n",
    "though with some modifications, it can also be used for solving regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ecb94-03bc-4336-8509-847d1296a098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6125b8e-4bdc-41b7-90a4-a3f64e9fb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. How do you handle categorical features in the Naive Approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d83a0-4651-42ec-b692-7b08ad0f1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = 1. Label Encoding:\n",
    "   - Label encoding assigns a unique numeric value to each category in a categorical feature.\n",
    "   - For example, if we have a feature \"color\" with categories \"red,\" \"green,\" and \"blue,\" label encoding could assign 0 to \"red,\" 1 to \"green,\" and 2 to \"blue.\"\n",
    "2. One-Hot Encoding:\n",
    "   - One-hot encoding creates binary dummy variables for each category in a categorical feature.\n",
    "   - For example, if we have a feature \"color\" with categories \"red,\" \"green,\" and \"blue,\" one-hot encoding would create three binary variables: \"color_red,\"\n",
    "    \"color_green,\" and \"color_blue.\"\n",
    "3. Count Encoding:\n",
    "   - Count encoding replaces each category with the count of its occurrences in the dataset.\n",
    "   - For example, if we have a feature \"city\" with categories \"New York,\" \"London,\" and \"Paris,\" count encoding would replace them with\n",
    "    the respective counts of instances belonging to each city.\n",
    "4. Binary Encoding:\n",
    "   - Binary encoding represents each category as a binary code.\n",
    "   - For example, if we have a feature \"country\" with categories \"USA,\" \"UK,\" and \"France,\" binary encoding would assign 00 to \"USA,\" 01 to \"UK,\" and 10 to \"France.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6fd060-e054-490e-b897-50dfe718b2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16afd8-39dd-455e-af25-52b2d1d4287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What is Laplace smoothing and why is it used in the Naive Approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d576561-9299-4176-8cd4-52364efc055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability\n",
    "in the Naïve Bayes machine learning algorithm. \n",
    "Using higher alpha values will push the likelihood towards a value of 0.5,\n",
    "i.e., the probability of a word equal to 0.5 for both the positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250ede1-7711-4208-8ed5-71f499bab341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c23e53-9d26-4d98-9b88-86662da6d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. How do you choose the appropriate probability threshold in the Naive Approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942cabb3-ef2c-48ee-aceb-bb1d763c83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = To keep it simple, in the case of binary classification, \n",
    "you can set the thresholds as a value in the range [0, 1] , such that they sum to 1 .\n",
    "This will get you the desired rule of Classify as True if the probability is over threshold T,\n",
    "otherwise classify as False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c302886-066b-492d-b1d7-6a473b15049e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3671fc-7ed8-44d8-aa48-187cf42164c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Give an example scenario where the Naive Approach can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41ba42-edf2-4f9a-ba13-ac94c4838214",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =  Naive Bayes Algorithm are sentimental analysis, classifying new articles, and spam filtration. \n",
    "Classification algorithms are used for categorizing new observations into predefined classes for the uninitiated data.\n",
    "The Naive Bayes Algorithm is known for its simplicity and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4a6ac-86b5-462a-b67e-0400cb3cd380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad17424-1d1e-4bc6-afb8-d277eb8f9f60",
   "metadata": {},
   "source": [
    "KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb98386-ac81-4703-83b7-142f119294de",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. What is the K-Nearest Neighbors (KNN) algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402dd08-e1b1-4f21-b0f3-8f94522dfb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = The K-Nearest Neighbors (KNN) algorithm is a supervised learning algorithm used \n",
    "for both classification and regression tasks. \n",
    "It is a non-parametric algorithm that makes predictions based on the similarity between\n",
    "the input instance and its K nearest neighbors in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe2b65-f594-4789-bec2-df18b45f4f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b33e-72f9-4ee6-b0bc-bc7f0c3225e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. How does the KNN algorithm work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb54c2-c853-4bcb-ba55-1a63695aee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANs = 1.First, the distance between the new point and each training point is calculated.\n",
    "2.The closest k data points are selected (based on the distance). ...\n",
    "3.The average of these data points is the final prediction for the new point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2b924-9621-4093-a165-0f13c42c79ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ae68f-dc7f-49a2-b0cc-05641640f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6dd23-1e16-40c9-ae38-e47c4aa35a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = The value of k is very crucial in the KNN algorithm to define the number of neighbors in the algorithm. \n",
    "The value of k in the k-nearest neighbors (k-NN) algorithm should be chosen based on the input data. \n",
    "If the input data has more outliers or noise, a higher value of k would be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb3f8d-dfa5-49a8-a355-b04dea02a022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3a93a-f513-4d38-b046-1a4fbb41ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "13. What are the advantages and disadvantages of the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2277f1-1b4b-419a-b232-6d3f67f2ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Advantages of KNN\n",
    "1.Quick calculation time.\n",
    "2.Simple algorithm – to interpret.\n",
    "3.Versatile – useful for regression and classification.\n",
    "4.High accuracy – you do not need to compare with better-supervised learning models.\n",
    "\n",
    "the disadvantages of using the k-nearest neighbors algorithm: \n",
    "   1. Associated computation cost is high as it stores all the training data.\n",
    "    2. Requires high memory storage. \n",
    "   3. Need to determine the value of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d6465-f8ab-42f1-88d8-b03478c493cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c559d5-b27e-47bf-bd10-b77bc579f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "14. How does the choice of distance metric affect the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee7c657-be2b-4188-9355-8c0de042b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = 1. Euclidean Distance:\n",
    "   - Euclidean distance is the most commonly used distance metric in KNN. \n",
    "It calculates the straight-line distance between two instances in the feature space.\n",
    "   - Euclidean distance works well when the feature scales are similar and\n",
    "    there are no specific considerations regarding the relationships between features.\n",
    "   \n",
    "\n",
    "2. Manhattan Distance:\n",
    "   - Manhattan distance, also known as city block distance or L1 norm, calculates the sum of absolute \n",
    "differences between corresponding feature values of two instances.\n",
    "   - Manhattan distance is more robust to outliers compared to Euclidean distance and is \n",
    "    suitable when the feature scales are different or when there are distinct feature dependencies.\n",
    "\n",
    "\n",
    "3. Minkowski Distance:\n",
    "   - Minkowski distance is a generalized form that includes both Euclidean distance and Manhattan distance as special cases.\n",
    "   - It takes an additional parameter, p, which determines the degree of the distance metric.\n",
    "    When p=1, it is equivalent to Manhattan distance, and when p=2, it is equivalent to Euclidean distance.\n",
    "\n",
    "\n",
    "4. Cosine Similarity:\n",
    "   - Cosine similarity measures the cosine of the angle between two vectors.\n",
    "It calculates the similarity based on the direction rather than the magnitude of the feature vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6586a8-e7b1-4cd1-8e3b-e4e16a29b6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752947b-2910-4bd2-93ad-be61a7cf54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "15. Can KNN handle imbalanced datasets? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c9d98-7170-41db-81df-ce069d4c3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Yes.\n",
    "     The more widely used techniques in the field of anomaly detection are based \n",
    "    on density techniques such as KNN local outlier factor, isolation forest, etc. \n",
    "    In general, the data is considered as a point in a multi-dimensional space,\n",
    "    defined by the number of features used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca9bef-0ca6-46d1-9dc0-1c43835f7d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e749df-e79c-4cf1-8f47-dfd99c5eb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "16. How do you handle categorical features in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a73e07-941a-44cb-9160-9e5c6b70e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = You have to decide how to convert categorical features to a numeric scale,\n",
    "and somehow assign inter-category distances in a way that makes sense with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ade720-19a9-4e09-965f-98f740b8356e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acab93d-fffc-4bf1-b245-39af0b064ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "17. What are some techniques for improving the efficiency of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69892f-ebcc-4e27-aa9a-523d2b55faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = In order to improve the efficiency and speed of KNN, reducing the dimensionality of the data is necessary. \n",
    "The curse of dimensionality can make the distance between data points less distinguishable and increase complexity for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e33c7c-9c0b-4b35-a4b6-3c07a872750a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323dccf-3c3d-4228-8e54-0aeec9a9f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "18. Give an example scenario where KNN can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d0ff1-1a77-4306-886f-e13a82bf2de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = With the help of KNN algorithms, we can classify a potential voter into\n",
    "various classes like “Will Vote”, “Will not Vote”, “Will Vote to Party 'Congress',\n",
    "“Will Vote to Party 'BJP'. Other areas in which KNN algorithm can be used are\n",
    "Speech Recognition, Handwriting Detection, Image Recognition and Video Recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949437f4-d755-401c-851d-8c411c423d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc3b48-f37c-454a-abd5-a112d8365c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f2ccc07-445c-4777-83db-bdca5b1bedf0",
   "metadata": {},
   "source": [
    "Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bce45f-f7cd-4703-b8cf-ba63617d11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "19. What is clustering in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e98536-13eb-4636-be56-d7a93767f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Clustering is the task of dividing the population or data points into a \n",
    "number of groups such that data points in the same groups are more similar to other\n",
    "data points in the same group and dissimilar to the data points in other groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b812d-c4c0-43d5-937b-ae0310bae442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda0475-180e-402b-a131-0b894e1dcb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "20. Explain the difference between hierarchical clustering and k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c9182-605f-476a-a211-3486c1564065",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = k-means is method of cluster analysis using a pre-specified no. of clusters.\n",
    "It requires advance knowledge of 'K'.\n",
    "Hierarchical clustering also known as hierarchical cluster analysis  is also a\n",
    "method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca56bfa-862b-4d13-9c50-30427eafa23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad36e3-2327-4841-85f6-6ede59f4ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "21. How do you determine the optimal number of clusters in k-means clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b44c6-32f4-411c-af48-8da17366d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = 1.Compute clustering algorithm (e.g., k-means clustering) for different values of k. ...\n",
    "2.For each k, calculate the average silhouette of observations (avg. ...\n",
    "3.Plot the curve of avg. ...\n",
    "4.The location of the maximum is considered as the appropriate number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81c358-1486-4997-b891-70ab3625c9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884a8da-9ae9-4a8c-b55c-7569258fa214",
   "metadata": {},
   "outputs": [],
   "source": [
    "22. What are some common distance metrics used in clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bd72d-8def-480d-a2c9-51fb600b3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Distance metrics are used in supervised and unsupervised learning to calculatesimilarity in data points. \n",
    "They improve the performance, whether thats for classification tasks or clustering.\n",
    "The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131b8c8-1a08-487e-a2cc-8da6727dd3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162ec34-ffa9-44de-8e77-2a650486f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "23. How do you handle categorical features in clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d86cb0-ebfb-4277-9ae4-717cce2d2d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Step 1: Pick K observations at random and use them as leaders/clusters.\n",
    "Step 2: Calculate the dissimilarities(no. of mismatches) and assign each observation to its closest cluster.\n",
    "Step 3: Define new modes for the clusters.\n",
    "Creating Toy Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e34b2-3512-45ed-bdef-41e27ecd6244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519d348-d54c-4e8d-8d72-3bd1b00b99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "24. What are the advantages and disadvantages of hierarchical clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705e9c9-6c78-4ccd-a243-0cc600a31da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = The advantage of Hierarchical Clustering is we dont have to pre-specify the clusters.\n",
    "However, \n",
    "it doesnt work very well on vast amounts of data or huge datasets. \n",
    "And there are some disadvantages of the Hierarchical Clustering algorithm that it is not suitable for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b6f9a-d35d-49fb-80a7-a335aaa1716b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e5e79-7368-4399-a4e0-f4026cfb3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "25. Explain the concept of silhouette score and its interpretation in clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa6beb-3ea0-47a6-9b95-e8dd3cbf68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = The silhouette value is a measure of how similar an object is to its own cluster compared to other clusters .\n",
    "The silhouette ranges from −1 to +1,\n",
    "where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48868e-075f-4322-9afb-d78a6931c656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b000a7e-ae97-496c-ae8f-3a2ddde09db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "26. Give an example scenario where clustering can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d44b1-16ba-44a5-b2b0-9704bae794da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Identifying Fake News.\n",
    "Fake news is not a new phenomenon, but it is one that is becoming prolific. \n",
    "Spam filter.\n",
    "Marketing and Sales. \n",
    "Classifying network traffic. \n",
    "Identifying fraudulent or criminal activity. \n",
    "Document analysis. \n",
    "Fantasy Football and Sports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4bb839-2577-4891-b187-f6115a1cceeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22f312a5-65eb-483e-9ad1-ea97f1d5eec8",
   "metadata": {},
   "source": [
    "Anomaly Detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84163379-6466-43dc-b4ae-a8975259d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "27. What is anomaly detection in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45981481-ceca-44b6-a5f3-43ea25f03c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Anomaly detection, also known as outlier detection,\n",
    "is the task of identifying patterns or instances that deviate significantly from \n",
    "the norm or expected behavior within a dataset.\n",
    "Anomalies are data points that differ from the majority of the data and may indicate unusual or suspicious behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1e7e7-c0b0-4134-90c9-2b7bd1eec2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313116bb-4c09-4ad8-af61-c202e94042ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "28. Explain the difference between supervised and unsupervised anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a776d14-7d12-43e5-a096-2a8ce55b6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = 1. Supervised Anomaly Detection:\n",
    "   - In supervised anomaly detection, the training dataset contains labeled instances, where each instance is labeled as either normal or anomalous.\n",
    "   - The algorithm learns from these labeled examples to classify new, unseen instances as normal or anomalous.\n",
    "   - Supervised anomaly detection typically involves the use of classification algorithms that are trained on labeled data.\n",
    "   - The algorithm learns the patterns and characteristics of normal instances and uses this knowledge to classify new instances.\n",
    "   - Supervised anomaly detection requires a sufficient amount of labeled data, including both normal and anomalous instances, for training.\n",
    "\n",
    "2. Unsupervised Anomaly Detection:\n",
    "   - In unsupervised anomaly detection, the training dataset does not contain any labeled instances.\n",
    "    The algorithm learns the normal behavior or patterns solely from the unlabeled data.\n",
    "   - The goal is to identify instances that deviate significantly from the learned normal behavior, considering them as anomalies.\n",
    "   - Unsupervised anomaly detection algorithms rely on the assumption that anomalies are rare and different from the majority of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573294c1-8846-4bc7-b3ba-b4699228f610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720ce9b-97cf-42d4-88b7-03b406404dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "29. What are some common techniques used for anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc4f56a-7b28-4921-9dec-1ab952ece827",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "   Supervised Machine Learning for Anomaly Detection\n",
    "    The most common supervised methods include Bayesian networks, k-nearest neighbors, \n",
    "    decision trees, supervised neural networks,and SVMs. \n",
    "    The advantage of supervised models is that they may offer a higher rate of detection than unsupervised techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740d126-9a55-4a80-ad3c-babeff220745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53857ce8-7148-4ef9-b5fa-9d8de765705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "30. How does the One-Class SVM algorithm work for anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9018a-612f-460d-afa4-8c4769612471",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     One-class SVM, or unsupervised SVM, is an algorithm used for anomaly detection. \n",
    "    The algorithm tries to separate data from the origin in the transformed high-dimensional predictor space.\n",
    "    ocsvm finds the decision boundary based on the primal form of SVM with the Gaussian kernel approximation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1834a-853b-48ad-b453-bf0f47ceca59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb861637-7e38-471e-adaf-b6ecbab88576",
   "metadata": {},
   "outputs": [],
   "source": [
    "31. How do you choose the appropriate threshold for anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ebfd6-bc1b-40cf-8c37-8430d4b994a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = 1.In the Upper Limit and Lower Limit fields, specify the upper and lower threshold limits.\n",
    "2.In the Consecutive Occurrences spinner, specify the number of consecutive threshold violations\n",
    "that must occur before an anomaly and an event is generated.\n",
    "3.In the Type drop-down list, specify the threshold type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea89c0-d6e2-4d25-b0c7-c7f67ba3bfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f829b3c-2e76-4b39-b28e-886edfad7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "32. How do you handle imbalanced datasets in anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc2f5a-2557-4a35-96aa-c03e0967a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Random Undersampling and Oversampling\n",
    "    A widely adopted and perhaps the most straightforward method for dealing with highly \n",
    "    imbalanced datasets is called resampling. \n",
    "    It consists of removing samples from the majority class  and/or \n",
    "    adding more examples from the minority class (over-sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f702eb8-9f6b-45cf-b872-db12f43c9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "33. Give an example scenario where anomaly detection can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ed12d-b054-44a0-9c3d-2fb6a7e2641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     Anomaly Detection Examples\n",
    "      For example, a credit card company will use anomaly detection to track how customers typically use their credit cards.\n",
    "        If a customer makes an abnormally large purchase or a purchase in a new location, \n",
    "        the algorithm recognizes the anomaly and alerts a team member to contact the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea6749-a29b-4a12-be91-2727fb154cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea91cf95-3adc-4d99-890a-634d2d9c132a",
   "metadata": {},
   "source": [
    "\n",
    "Dimension Reduction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a7e5c-a4e4-4324-afc4-73a567a10b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "34. What is dimension reduction in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9503f0d-043e-4a74-9727-7c4ed99b41bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Dimensionality reduction is a technique used in machine learning to reduce the \n",
    "     number of input features or variables while preserving the most relevant information.\n",
    "     It aims to simplify the data representation, remove noise or irrelevant features, \n",
    "    and improve computational efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd5921-1a7e-4e80-aab8-a25a5cb6f7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4432242a-1622-4a72-b751-3e2ccc1b771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "35. Explain the difference between feature selection and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a53be-84f6-4d87-9d0d-f45f75d66d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Feature selection techniques are used when model explainability is a key requirement.\n",
    "      Feature extraction techniques can be used to improve the predictive performance of the models,\n",
    "      especially, in the case of algorithms that dont support regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bbc61-f8c1-43a8-a361-529022445b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2897b-c58d-4552-9601-136c8e8174a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "36. How does Principal Component Analysis (PCA) work for dimension reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee2201a-fb3e-4dbc-8ded-16381dc81086",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "    PCA helps us to identify patterns in data based on the correlation between features.\n",
    "    In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional\n",
    "    data and projects it onto a new subspace with equal or fewer dimensions than the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1cf6f-8dfd-4017-acb9-1524a449869b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068a599-880c-424b-aaa8-0aae5c7eecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "37. How do you choose the number of components in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a515656-7244-4719-b2dd-4ef2d4ee3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = 1. Variance Explained:\n",
    "   - Calculate the cumulative explained variance ratio for each principal component. \n",
    "     This indicates the proportion of total variance captured by including that component.\n",
    "    2. Elbow Method:\n",
    "   - Plot the explained variance as a function of the number of components. \n",
    "      Look for an \"elbow\" point where the explained variance starts to level off.\n",
    "   3. Scree Plot:\n",
    "   - Plot the eigenvalues of the principal components in descending order. \n",
    "      Look for a point where the eigenvalues drop sharply, indicating a significant drop in explained variance\n",
    "   4. Cross-validation:\n",
    "   - Use cross-validation techniques to evaluate the performance of the PCA with different numbers of components.\n",
    "     Select the number of components that maximizes a performance metric, such as model accuracy or mean squared error, on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da52703-0d1f-4291-a6f3-63fb1281ff5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca373b68-881a-41cd-a526-4452b1a8ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "38. What are some other dimension reduction techniques besides PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f3be7-ab0f-489b-947d-fa94030a55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = 1.Feature selection. \n",
    "    2.Feature extraction. \n",
    "    3.Principal Component Analysis (PCA) \n",
    "    4.Non-negative matrix factorization (NMF).\n",
    "    5.Linear discriminant analysis (LDA) .\n",
    "    6.Generalized discriminant analysis (GDA) .\n",
    "    7.Missing Values Ratio. \n",
    "    8.Low Variance Filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2cc0b-b5c8-455c-9b28-fae2b374ab5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a8cd6-5c7e-4be8-807c-6c03cd79110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "39. Give an example scenario where dimension reduction can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b956d37-6e87-42c6-848e-764db778a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = example,\n",
    "     maybe we can combine Dum Dums and Blow Pops to look at all lollipops together. \n",
    "     Dimensionality reduction can help in both of these scenarios.\n",
    "    There are two key methods of dimensionality reduction: Feature selection:\n",
    "    Here, we select a subset of features from the original feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a001531-742a-43c2-930d-19948199590a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36101a0b-4e07-40f7-a726-4d8ff0244a2d",
   "metadata": {},
   "source": [
    "Feature Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5bc439-5f1c-401f-9218-8bcd509bf501",
   "metadata": {},
   "outputs": [],
   "source": [
    "40. What is feature selection in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e50866-185d-4214-ba88-748d0e95e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Feature selection is the process of selecting a subset of relevant features from a larger \n",
    "    set of available features in a machine learning dataset.\n",
    "    The goal of feature selection is to improve model performance, reduce complexity, \n",
    "    enhance interpretability, and mitigate the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80549c06-ff81-4262-82f9-65871d8e218e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111644f9-b667-41f9-86a9-09ee23d47d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "41. Explain the difference between filter, wrapper, and embedded methods of feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25a2a2-ef5e-4e03-a8f5-a69e91294069",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Filter methods perform the feature selection independently of construction of the classification model.\n",
    "     Wrapper methods iteratively select or eliminate a set of features using the prediction accuracy of the classification model. \n",
    "     In embedded methods the feature selection is an integral part of the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c759ee-7f57-48cc-9f6a-252efb7895a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8139d67-31a9-4f86-8008-68886784c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "42. How does correlation-based feature selection work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e9d54-36af-4d47-8de2-d95a42e62c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = How does correlation help in feature selection? Features with high correlation are more linearly dependent and\n",
    "      hence have almost the same effect on the dependent variable. \n",
    "      So, when two features have high correlation, we can drop one of the two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411887b1-9615-4303-9643-49ea61a7ee31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df02543-1ab5-4492-8a4d-3e1beb6d49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "43. How do you handle multicollinearity in feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8d381-68c5-43ab-898c-70362c8cc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = To address multicollinearity, techniques such as regularization or feature \n",
    "     selection can be applied to select a subset of independent variables that are\n",
    "      not highly correlated with each other.  \n",
    "    In this article, we will focus on the most common one – VIF ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eed93f-3c54-4dc0-8e07-0d6d5eae500f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a55f2d-6985-4043-bb51-87a916ff65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "44. What are some common feature selection metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8ee0f-b90a-461e-99e9-7619ab34d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =1. Chi-square Test. The Chi-square test is used for categorical features in a dataset. \n",
    "    2.Fisher's Score. \n",
    "    3.Correlation Coefficient. \n",
    "    4.Dispersion Ratio. \n",
    "    5.Backward Feature Elimination. \n",
    "    6.Recursive Feature Elimination. \n",
    "    7.Random Forest Importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810fbda-7908-484f-8741-ed97171dbb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028dc884-1abb-4665-aec3-e42d4a01f32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "45. Give an example scenario where feature selection can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a511df4-79ab-48f0-8636-bc9afbbdf6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = 1. Dimensionality Reduction: Text data often results in high-dimensional feature spaces,\n",
    "      where each word or term becomes a feature.\n",
    "     The high dimensionality can lead to computational inefficiency and the curse of dimensionality.\n",
    "    Feature selection allows you to reduce the number of features, focusing on the most relevant ones, \n",
    "    thereby simplifying the model and improving computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224809a9-9668-4099-9845-f0769fbc106c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73a404fb-0bcd-4f8c-a6f7-ae45406691d9",
   "metadata": {},
   "source": [
    "Data Drift Detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791aa5e6-2e23-4df3-afd5-6f04b2b0e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "46. What is data drift in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b730e8-71f5-4899-833d-c863259004f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Data drift refers to the phenomenon where the statistical properties of the target variable or input features change over time,\n",
    "    leading to a degradation in model performance. \n",
    "    It is important to monitor and address data drift in machine learning because models trained on \n",
    "    historical data may become less accurate or unreliable when deployed in production environments where \n",
    "    the underlying data distribution has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcabf819-0f38-42ef-9689-b77ae8bfdb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd504b9f-8efa-453e-a1c7-5e15f835c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "47. Why is data drift detection important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbb222-6ef8-42e9-9be9-dc3d4f2fec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Data drift is one of the top reasons model accuracy degrades over time.\n",
    "   For machine learning models, data drift is the change in model input data that\n",
    "    leads to model performance degradation.\n",
    "    Monitoring data drift helps detect these model performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e494c2b-8a35-4ce9-ba1d-6cf40771547c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf047e6-17f4-4675-a256-d32947efc641",
   "metadata": {},
   "outputs": [],
   "source": [
    "48. Explain the difference between concept drift and feature drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a535212-7d02-4bde-9141-52680a316925",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Data drift refers to the changing distribution of the data to which the model is applied.\n",
    "    Concept drift refers to a changing underlying goal or objective for the model.\n",
    "    Both data drift and concept drift can lead to a decline in the performance of a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5983b-a180-4ace-b0b3-3c0b0c2ab8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e1d31-393c-46a3-80a4-82e77f5debe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "49. What are some techniques used for detecting data drift?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1064972-258b-4b3f-9b5e-3ca1237182e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Time distribution-based methods use statistical methods to calculate the difference between\n",
    "    two probability distributions to detect drift. \n",
    "    These methods include the Population Stability Index, \n",
    "    KL Divergence, JS Divergence, KS Test, and the Wasserstein Metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212d692-2a06-45cd-9a0b-5f9a81acfdd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4abb69-33db-4e0b-a58d-775248fab9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "50. How can you handle data drift in a machine learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f87a82-3948-41a3-b8c9-5453fbdc3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Some strategies for addressing drift include continuously monitoring and evaluating the performance of a model,\n",
    "    updating the model with new data, and using machine learning models that are more robust to drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03654d1f-4111-4625-b261-b132b6f2f23f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "588d2ef7-65cf-4ce4-80e3-b475ed5eaea2",
   "metadata": {},
   "source": [
    "Data Leakage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a1687-73fd-4bbd-b33e-f4122b7777cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "51. What is data leakage in machine learning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22813724-8f13-45f4-a9d8-562c79a36526",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Data leakage refers to the unintentional or improper inclusion of information from the training\n",
    "   data that should not be available during the models deployment or evaluation.\n",
    "    It occurs when there is a contamination of the training data with information that\n",
    "    is not realistically obtainable at the time of prediction or when evaluating model performance. \n",
    "    Data leakage can significantly impact the accuracy and reliability of machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85259c86-6f50-45de-8453-f73b6d9249a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d9547-4652-494a-8c38-fe9011956b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "52. Why is data leakage a concern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73861dc2-749e-4b48-9789-cf52bb30d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = A data leak is when information is exposed to unauthorized people due to internal errors.\n",
    "   This is often caused by poor data security and sanitization, outdated systems, or a lack of employee training.\n",
    "    Data leaks could lead to identity theft, data breaches, or ransomware installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de02dea-d91d-4683-9239-eaa3507e0a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4b0e3-d65d-45bd-8825-a5fff55cc3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "53. Explain the difference between target leakage and train-test contamination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c8de98-3c61-41ed-a8f6-72c94f24edeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Target Leakage:\n",
    "- Target leakage refers to the situation where information from the target variable is unintentionally included in the feature set. \n",
    "  This means that the feature includes data that would not be available at the time of making predictions in real-world scenarios.\n",
    "- Target leakage leads to inflated performance during model training and evaluation because the model has access to information that\n",
    "  it would not realistically have during deployment.\n",
    "\n",
    "    Train-Test Contamination:\n",
    "- Train-test contamination occurs when information from the test set (unseen data) leaks into the training set (used for model training).\n",
    "- Train-test contamination leads to overly optimistic performance estimates during model development because the model has \"seen\" the test \n",
    "  data and can learn from it, which is not representative of real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a42a4-0bbc-42c9-b6ba-fb3d13c6b145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764a407-a092-4cd6-a597-3ede6e51bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "54. How can you identify and prevent data leakage in a machine learning pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5608079-7b49-4645-ad6e-9925215e54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = We can also minimize the data leakage problem by adding a validation set to both training and test data sets.\n",
    "   Further, the validation set also helps identify the overfitting, which acts as\n",
    "    a caution warning when deploying predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706051a6-4ad3-4c26-a17e-82f04f0e16b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4d8b3-3edb-41f9-bbe9-d82de6fa4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "55. What are some common sources of data leakage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b52fe4-531a-420f-b818-a29295dcfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = 1. Target Leakage: Including features that are derived from information that would not be available at the time of prediction. \n",
    "     For example, including future information or data that is influenced by the target variable can lead to target leakage.\n",
    "\n",
    "2. Time-Based Leakage: Incorporating time-dependent information that should not be available during prediction. \n",
    "   This can happen when using future values or time-dependent features that reveal future information.\n",
    "\n",
    "3. Data Preprocessing: Improperly applying preprocessing steps to the entire dataset before splitting into train and test sets.\n",
    "   This can include scaling, normalization, or other transformations that introduce information from the test set into the training set.\n",
    "\n",
    "4. Train-Test Contamination: Inadvertently using information from the test set during feature engineering, model selection,\n",
    "   or hyperparameter tuning. This can happen when the test set is accidentally accessed or when information leaks from the test set\n",
    "    into the training set.\n",
    "\n",
    "5. Data Transformation: Using data-driven transformations or encodings based on the entire dataset, including information that\n",
    "   is not available during prediction. This can introduce biases and lead to overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35dac7a-d688-431a-bcaa-4580853b7efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7b7bc-8c6e-47d5-8a17-9a8025c20d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "56. Give an example scenario where data leakage can occur Cross Validation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6901fcb9-5c98-40e4-9d0e-0232057ffcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =  if you need to perform variable imputation, but the imputer is fit on the whole dataset before \n",
    "   splitting the data into training and testing, information from the distribution of the validation \n",
    "    set is leaking into the imputer,\n",
    "    so the validation performance is likely to be higher than what it would actually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6e073-1b9c-43a7-bf30-a589bfb85bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4869ee8-8e78-4698-8c69-640ef0a39707",
   "metadata": {},
   "outputs": [],
   "source": [
    "57. What is cross-validation in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ed796-308e-4b69-b5b8-d19820e24e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available \n",
    "   input data and evaluating them on the complementary subset of the data.\n",
    "    Use cross-validation to detect overfitting, ie, failing to generalize a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a00737-3006-462e-84df-3aa8b0c95080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee05ead-7fe7-4f15-babe-af6a3c74a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "58. Why is cross-validation important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ccaed-cc9a-4595-a38a-6be308785d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = The main advantage of cross-validation is that it provides an estimate of the performance of the model on new data,\n",
    "   which is important for assessing the models generalizability.\n",
    "    It also helps to avoid overfitting, which is a common problem in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27f6c3-2211-4860-954f-34a20b59977c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da7a8f-5951-4031-a584-852074a10e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df3afd-c6eb-4d8f-8875-4e9152eadef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = The difference between GroupKFold and StratifiedGroupKFold is that the former attempts to create balanced folds\n",
    "   such that the number of distinct groups is approximately the same in each fold,\n",
    "    whereas StratifiedGroupKFold attempts to create folds which preserve the percentage of samples for each class as much as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad2016-48a3-4906-9c02-d5563b00ffd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044ae89-3ae5-4426-8650-4399a5014f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "60. How do you interpret the cross-validation results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412e958-8a92-457b-9bdd-93762534a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Cross validation is a technique that allows us to produce test set like scoring metrics using the training set.\n",
    "   That is, it allows us to simulate the effects of “going out of sample” using just our training data, \n",
    "    so we can get a sense of how well our model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b96e56-09b7-4a21-afd3-c631a0fc7043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
